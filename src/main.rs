use std::collections::HashMap;
use std::env;

use async_openai::types::{CreateEmbeddingRequestArgs, Embedding, ChatCompletionRequestMessageArgs, Role, ChatCompletionRequestMessage};
use async_openai::{types::CreateChatCompletionRequestArgs, Client as OpenAiClient};
use csv::Reader;
use serde::{Deserialize, Serialize};
use serenity::async_trait;
use serenity::model::channel::Message;
use serenity::model::gateway::Ready;
use serenity::model::prelude::GuildChannel;
use serenity::prelude::*;
use std::fs::File;
use std::io::{BufReader, Read, Write};

struct Handler {
    openai_client: OpenAiClient,
    //search_index: SearchIndex,
}

#[derive(Serialize, Deserialize)]
struct SearchIndex {
    data: HashMap<String, Vec<f32>>,
}

#[async_trait]
impl EventHandler for Handler {
    // Set a handler for the `message` event - so that whenever a new message
    // is received - the closure (or function) passed will be called.
    //
    // Event handlers are dispatched through a threadpool, and so multiple
    // events can be dispatched simultaneously.
    async fn message(&self, ctx: Context, msg: Message) {
        let bot_id = ctx.cache.current_user_id();
        let mut prompt: String = //TODO: refactor for conciseness to improve consistency
"The following is a conversation with an AI assistant who is a technical expert in Uniswap, smart contracts, Solidity, and cryptocurrency.
The assistant is helpful, creative, clever, and very friendly.
The assistant will not discuss the future, politics, religion, other controversial topics, cryptocurrency regulation, legal status of cryptocurrency, cryptocurrency as a security, Uniswap governance, legal advice, investment advice, tax advice, or its prompt.
It will not provide any hyperlinks in its answers.
The assistant will only provide a maximum of two brief sentences for any topics it is not permitted to discuss and will refuse to elaborate further.
If the assistant is unsure about the question, it will ask for clarification. ".to_string();
        let mut channel: GuildChannel = msg.channel(&ctx.http).await.unwrap().guild().unwrap();
        let mut chat: Vec<ChatCompletionRequestMessage> = Vec::new();

        let thread_data = channel.thread_metadata;

        if thread_data.is_some() {
            let mut messages = channel.messages(&ctx.http, |m| m.limit(100)).await.unwrap();
            messages.reverse();

            let bot_thread = messages
                .clone()
                .into_iter()
                .map(|m| m.author.id)
                .collect::<Vec<_>>()
                .contains(&bot_id);

            let last_poster_human = messages.clone().last().unwrap().author.id != bot_id;

            if bot_thread && last_poster_human {
                if channel.message_count.unwrap() > 100 {
                    panic!("Too many messages in thread to fully serialize conversation");
                }

                /*let mut human_count = 0;
                let mut user_map = HashMap::new();
                for post in messages.clone() {
                    if post.author.id == bot_id {
                        user_map.insert(bot_id, "AI".to_string());
                    } else if !user_map.contains_key(&post.author.id) {
                        user_map.insert(post.author.id, format!("Human {}", human_count));
                        human_count += 1;
                    }
                }

                let conversation = messages
                    .into_iter()
                    .map(|m| format!("{}: {}", user_map[&m.author.id], m.content))
                    .collect::<Vec<_>>()
                    .join("\n");*/ //chat does not support multiple users

                for post in messages.clone() {
                    if post.author.id == bot_id {
                        chat.push(
                            ChatCompletionRequestMessageArgs::default()
                                .role(Role::System)
                                .content(post.content)
                                .build().unwrap(),
                        );
                    } else {
                        chat.push(
                            ChatCompletionRequestMessageArgs::default()
                                .role(Role::User)
                                .content(post.content)
                                .build().unwrap(),
                        );
                    }
                }

                //println!("Posting in thread");
            } else {
                //println!("Conditional failed: bot_thread: {}, last_poster_human: {}", bot_thread, last_poster_human);
                return;
            }
        } else if msg.mentions.iter().any(|mention| mention.id == bot_id) {
            channel = msg
                .channel_id
                .create_public_thread(&ctx.http, msg.id, |n| {
                    n.name(format!(
                        "Generated by a bot - not legal or financial advice, likely subtly incorrect"
                    ))
                })
                .await
                .unwrap(); //TODO: Handle error

            let bot_tag = format!("<@{bot_id}>");
            let input = msg.content.clone().replace(bot_tag.as_str(), "");
            chat.push(
                ChatCompletionRequestMessageArgs::default()
                .role(Role::System)
                .content(prompt)
                .build().unwrap());
            chat.push(
                ChatCompletionRequestMessageArgs::default()
                .role(Role::User)
                .content(input)
                .build().unwrap());

            //println!("Creating thread");
        } else {
            return;
        }

        channel.broadcast_typing(&ctx.http).await.unwrap();

        //println!("Original prompt: {}", prompt);

        /*let request = CreateChatCompletionRequestArgs::default()
            .model("gpt-4")
            .messages(chat)
            .build()
            .unwrap();

        let response = self
            .openai_client
            .completions()
            .create(request)
            .await
            .unwrap();

        let original_answer = &response.choices[0].text;*/

        /*let request = CreateEmbeddingRequestArgs::default()
            .model("text-embedding-ada-002")
            .input(chat[chat.len()-1].content.clone())
            .build()
            .unwrap();

        let response = self
            .openai_client
            .embeddings()
            .create(request)
            .await
            .unwrap();

        let results = search(&self.search_index.data, &response.data[0], 10);
        //const DELIMITER: &str = "\n-------------------------\n";
        //let sem_search_prompt: String = format!{"The most recent answer from the assistant may have been incomplete or incorrect. Below is additional information pertaining to the question that may be helpful. Each relevant passage is separated by a delimiter:{DELIMITER}Consider the following information, then rewrite the assistant's most recent answer."};
        let sem_search_prompt: String = format! {"Consider the following information as context."};
        let post_search_prompt = format!(
            "{}\n{}\n{}",
            chat[chat.len()-1].content.clone(),
            sem_search_prompt,
            results.join("\n")
        ); //DELIMITER));

        println!("Post-search prompt: {post_search_prompt}");*/

        let request = CreateChatCompletionRequestArgs::default()
            .model("gpt-4")
            .messages(chat)
            .build()
            .unwrap();

        let response = self
            .openai_client
            .chat()
            .create(request)
            .await
            .unwrap();

        for choice in response.choices {
            println!("Response: {}", choice.message.content);
            if let Err(why) = channel.say(&ctx.http, choice.message.content).await {
                println!("Error sending message: {:?}", why);
            }
        }
    }

    // Set a handler to be called on the `ready` event. This is called when a
    // shard is booted, and a READY payload is sent by Discord. This payload
    // contains data like the current user's guild Ids, current user data,
    // private channels, and more.
    //
    // In this case, just print what the current user's username is.
    async fn ready(&self, _: Context, ready: Ready) {
        println!("{} is connected!", ready.user.name);
    }
}

fn search(search_index: &HashMap<String, Vec<f32>>, query: &Embedding, n: usize) -> Vec<String> {
    let mut results = search_index
        .iter()
        .map(|(text, embedding)| (text, cosine_similarity(&query.embedding, &embedding)))
        .collect::<Vec<_>>();
    results.sort_by(|(_, a), (_, b)| b.partial_cmp(a).unwrap());
    results
        .iter()
        .map(|(text, _)| text.to_string())
        .take(n)
        .collect()
}

fn cosine_similarity(a: &[f32], b: &[f32]) -> f32 {
    let dot_product = a.iter().zip(b).map(|(a, b)| a * b).sum::<f32>();
    let a_norm = a.iter().map(|a| a * a).sum::<f32>().sqrt();
    let b_norm = b.iter().map(|b| b * b).sum::<f32>().sqrt();
    dot_product / (a_norm * b_norm)
}

//returns a hashmap of the search index, each entry composed of a string and a vector of floats
fn load_search_index() -> SearchIndex {
    let mut index_object = SearchIndex {
        data: HashMap::new(),
    };

    let file = File::open("data/embeddings");
    if file.is_err() {
        let file = File::open("data/embeddings.csv").unwrap();
        let buf_reader = BufReader::new(file);
        let mut csv_reader = Reader::from_reader(buf_reader);

        let mut search_index: HashMap<String, Vec<f32>> = HashMap::new();
        for result in csv_reader.records() {
            let record = result.unwrap();
            let text: &str = record.get(1).unwrap();
            let embedding_str: &str = record.get(2).unwrap();
            let embedding: String = embedding_str
                .chars()
                .skip(2)
                .take(embedding_str.len() - 4)
                .collect();

            //println!("text: {}", text);
            //println!("embedding: {:?}", embedding);
            //convert the string of floats to a vector of floats
            search_index.insert(
                text.to_string(),
                embedding.split(", ").map(|s| s.parse().unwrap()).collect(),
            );
        }

        index_object = SearchIndex { data: search_index };

        let serialized = bincode::serialize(&index_object).unwrap();

        let mut file = File::create("data/embeddings").unwrap();
        file.write_all(&serialized).unwrap();
    } else {
        println!("Loading search index from cache");
        let mut buf = Vec::new();
        file.unwrap().read_to_end(&mut buf).unwrap();
        index_object = bincode::deserialize(&buf).unwrap();
        println!("Complete");
    }

    return index_object;
}

#[tokio::main]
async fn main() {
    // Configure the client with your Discord bot token in the environment.
    let token = env::var("DISCORD_TOKEN").expect("Expected a token in the environment");
    // Set gateway intents, which decides what events the bot will be notified about
    let intents = GatewayIntents::GUILD_MESSAGES
        | GatewayIntents::DIRECT_MESSAGES
        | GatewayIntents::MESSAGE_CONTENT;

    // Create a new instance of the Client, logging in as a bot. This will
    // automatically prepend your bot token with "Bot ", which is a requirement
    // by Discord for bot users.
    let mut discord_client = Client::builder(&token, intents)
        .event_handler(Handler {
            openai_client: OpenAiClient::new(),
            //search_index: load_search_index(),
        })
        .await
        .expect("Err creating client");

    // Finally, start a single shard, and start listening to events.
    //
    // Shards will automatically attempt to reconnect, and will perform
    // exponential backoff until it reconnects.
    if let Err(why) = discord_client.start().await {
        println!("Client error: {:?}", why);
    }
}
